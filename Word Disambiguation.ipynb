{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42b65f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.spatial.distance import cosine\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a90adf44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe word embeddings.\n",
    "model = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77a0d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words . Word Stemming . Return new tokenised list.\n",
    "def filter_sentence(sentence):\n",
    "    filtered_sent = []\n",
    "    filtered_dict = set()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    for w, pos in tagged_words:\n",
    "        if pos.startswith('NN') and w not in stop_words:\n",
    "            new = lemmatizer.lemmatize(ps.stem(w))\n",
    "            if new not in filtered_dict:\n",
    "                word_vec = get_vector_or_none(new)\n",
    "                if word_vec is not None:\n",
    "                    filtered_sent.append(word_vec)\n",
    "                filtered_dict.add(new)\n",
    "                for i in create_sym(w):\n",
    "                    if i not in filtered_dict:\n",
    "                        sym_vec = get_vector_or_none(i)\n",
    "                        if sym_vec is not None:\n",
    "                            filtered_sent.append(sym_vec)\n",
    "                        filtered_dict.add(i)\n",
    "\n",
    "    return filtered_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e8e8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add synonyms to match list\n",
    "def create_sym(word):\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            synonyms.append(i.name())\n",
    "\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ec9d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_or_none(word):\n",
    "    try:\n",
    "        word_vec = model[word]\n",
    "        return word_vec\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58c0bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sim(w1_vec, w2_vec):\n",
    "    # Get the vector representations of the two words.\n",
    "    return 1 - cosine(w1_vec, w2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fd3aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WSD_Test_Tissue(sentence):\n",
    "    target_sent = sentence\n",
    "    file1 = codecs.open(\"tissue_organ.txt\", 'r', 'utf-8')\n",
    "    sent1 = file1.read().lower()\n",
    "    file2 = codecs.open(\"tissue_paper.txt\", 'r', \"utf-8\")\n",
    "    sent2 = file2.read().lower()\n",
    "\n",
    "    filtered_sent1 = filter_sentence(sent1)\n",
    "    filtered_sent2 = filter_sentence(sent2)\n",
    "    filtered_target = filter_sentence(target_sent)\n",
    "\n",
    "    target_1_similarity = 0\n",
    "    target_2_similarity = 0\n",
    "    \n",
    "    for i in filtered_target:\n",
    "        for j in filtered_sent1:\n",
    "            target_1_similarity = target_1_similarity + check_sim(i, j)\n",
    "\n",
    "        for j in filtered_sent2:\n",
    "            target_2_similarity = target_2_similarity + check_sim(i, j)\n",
    "    \n",
    "    if target_1_similarity > target_2_similarity:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43f54850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WSD_Test_Rubbish(sentence):\n",
    "    target_sent = sentence\n",
    "    file1 = codecs.open(\"rubbish_trash.txt\", 'r', 'utf-8')\n",
    "    sent1 = file1.read().lower()\n",
    "    file2 = codecs.open(\"rubbish_bull.txt\", 'r', \"utf-8\")\n",
    "    sent2 = file2.read().lower()\n",
    "\n",
    "    filtered_sent1 = filter_sentence(sent1)\n",
    "    filtered_sent2 = filter_sentence(sent2)\n",
    "    filtered_target = filter_sentence(target_sent)\n",
    "\n",
    "    target_1_similarity = 0\n",
    "    target_2_similarity = 0\n",
    "    \n",
    "    for i in filtered_target:\n",
    "        for j in filtered_sent1:\n",
    "            target_1_similarity = target_1_similarity + check_sim(i, j)\n",
    "\n",
    "        for j in filtered_sent2:\n",
    "            target_2_similarity = target_2_similarity + check_sim(i, j)\n",
    "    \n",
    "    if target_1_similarity > target_2_similarity:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8c65b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WSD_Test_Yarn(sentence):\n",
    "    target_sent = sentence\n",
    "    file1 = codecs.open(\"yarn_recital.txt\", 'r', 'utf-8')\n",
    "    sent1 = file1.read().lower()\n",
    "    file2 = codecs.open(\"yarn_thread.txt\", 'r', \"utf-8\")\n",
    "    sent2 = file2.read().lower()\n",
    "\n",
    "    filtered_sent1 = filter_sentence(sent1)\n",
    "    filtered_sent2 = filter_sentence(sent2)\n",
    "    filtered_target = filter_sentence(target_sent)\n",
    "\n",
    "    target_1_similarity = 0\n",
    "    target_2_similarity = 0\n",
    "    \n",
    "    for i in filtered_target:\n",
    "        for j in filtered_sent1:\n",
    "            target_1_similarity = target_1_similarity + check_sim(i, j)\n",
    "\n",
    "        for j in filtered_sent2:\n",
    "            target_2_similarity = target_2_similarity + check_sim(i, j)\n",
    "    \n",
    "    if target_1_similarity > target_2_similarity:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c365925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense 1\n",
      "Time:  3.2946856999999454\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "sense1 = [\n",
    "    \"The tissue in his injured leg was slowly healing over time.\",\n",
    "    \"The doctor examined the tissue under a microscope to look for abnormalities.\",\n",
    "    \"She donated tissue for research purposes to help find a cure for the disease.\",\n",
    "    \"The plant's tissue was affected by the harsh weather conditions and began to wilt.\",\n",
    "    \"The tumor was growing in the brain tissue and causing severe symptoms.\",\n",
    "#     \"He studied the tissue samples to learn more about the genetic makeup of the organism.\",\n",
    "#     \"The tissue in the heart is responsible for pumping blood throughout the body.\",\n",
    "#     \"She suffered a tear in the muscle tissue during the workout.\",\n",
    "#     \"The biopsy revealed abnormal tissue growth that required further examination.\",\n",
    "#     \"The tissue lining the stomach protects it from the acidic environment.\",\n",
    "#     \"The tissue around the wound was swollen and inflamed.\",\n",
    "#     \"The disease attacks the nervous tissue in the brain and spinal cord.\",\n",
    "#     \"The tissue in the bone marrow produces new blood cells.\",\n",
    "#     \"The bird's beak is made of a tough tissue that can break through hard shells.\",\n",
    "#     \"The fruit's tissue contains a high amount of water and nutrients.\",\n",
    "#     \"The tumor was successfully removed without damaging surrounding tissue.\",\n",
    "#     \"He needed surgery to repair the damaged tissue in his knee.\",\n",
    "#     \"The tissue in the liver filters and processes toxins from the body.\",\n",
    "#     \"The flower's tissue is delicate and easily damaged by strong winds.\",\n",
    "#     \"The organism's tissue was able to regenerate after being injured.\",\n",
    "#     \"The biopsy showed that the tissue was cancerous and required immediate treatment.\",\n",
    "#     \"The tissue in the lungs is responsible for exchanging oxygen and carbon dioxide.\",\n",
    "#     \"The tissue in the skin helps to regulate body temperature and protect against pathogens.\",\n",
    "#     \"The tissue in the eyes allows us to see by transmitting visual information to the brain.\",\n",
    "#     \"The athlete strained the soft tissue in his ankle and had to sit out the game.\",\n",
    "#     \"The tumor was located in the connective tissue and required specialized treatment.\",\n",
    "#     \"The tissue in the blood vessels is responsible for regulating blood pressure and flow.\",\n",
    "#     \"The tissue in the intestines absorbs nutrients from food and eliminates waste.\",\n",
    "#     \"The tissue in the kidneys filters waste and excess water from the blood.\",\n",
    "#     \"The organism's tissue was damaged by exposure to radiation.\",\n",
    "]\n",
    "\n",
    "# sense2 = [\n",
    "#     \"I used tissue paper to wrap the gift.\",\n",
    "#  \"She blew her nose with a tissue.\",\n",
    "#  \"He wiped his tears away with a tissue.\",\n",
    "#  \"The tissue was soft and absorbent.\",\n",
    "#  \"She dabbed some perfume on her wrist with a tissue.\",\n",
    "#  \"I always keep a pack of tissue paper in my bag.\",\n",
    "#  \"He carefully cleaned the wound with a tissue.\",\n",
    "#  \"She wiped off her lipstick with a tissue.\",\n",
    "#  \"The tissue stuck to his sweaty forehead.\",\n",
    "#  \"She folded the tissue neatly and put it away.\",\n",
    "#  \"I threw the used tissue into the trash.\",\n",
    "#  \"He sneezed into a tissue and threw it away.\",\n",
    "#  \"She used a tissue to clean the smudges off her glasses.\",\n",
    "#  \"I unfolded the tissue and saw a message written on it.\",\n",
    "#  \"The tissue was too thin and tore easily.\",\n",
    "#  \"She crumpled the tissue in her hand and threw it at him.\",\n",
    "#  \"The tissue floated away in the wind.\",\n",
    "#  \"He used a tissue to blot the excess oil from his face.\",\n",
    "#  \"She carefully wrapped the delicate object with tissue paper.\",\n",
    "#  \"I wiped the spilled coffee off the table with a tissue.\",\n",
    "#  \"He pulled out a tissue and blew his nose loudly.\",\n",
    "#  \"She used a tissue to wipe the sweat from her forehead.\",\n",
    "#  \"The tissue box was empty and needed to be refilled.\",\n",
    "#  \"I tore off a piece of tissue and used it to clean the spilled sauce.\",\n",
    "#  \"He nervously wiped his palms with a tissue.\",\n",
    "#  \"She held the tissue to her mouth to stifle her sobs.\",\n",
    "#  \"The tissue was scented with lavender.\",\n",
    "#  \"I balled up the tissue and threw it in the trash can.\",\n",
    "#  \"He wrapped the fragile ornament with tissue paper to protect it during transport.\",\n",
    "#  \"She used a tissue to dab at the sweat on her upper lip.\",\n",
    "#  \"The tissue disintegrated when it was wet.\"\n",
    "# ]\n",
    "\n",
    "def count_ones(arr):\n",
    "    count = 0\n",
    "    for val in arr:\n",
    "        if val == 1:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(\"Sense 1\")\n",
    "sense_1_results = []\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "for sent in sense1:\n",
    "    sense_1_results.append(WSD_Test_Tissue(sent))\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  \n",
    "print(f\"accuracy: {count_ones(sense_1_results)/len(sense_1_results)}\")\n",
    "\n",
    "# print(\"Sense 2\")\n",
    "# sense_2_results = []\n",
    "# for sent in sense2:\n",
    "#     sense_2_results.append(WSD_Test_Tissue(sent))\n",
    "# print(f\"accuracy: {(len(sense_2_results)-count_ones(sense_2_results))/len(sense_2_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09abf657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we charge by the amount of space that rubbish takes up. \n",
      "after the 9/11 attacks, much of the debris was taken to fresh kills - the former rubbish dump for the city.\n",
      "bring back the days when we used to talk rubbish about men, sex and any other silly subject that made us laugh. \n",
      "the three-part recycling bin makes easy work of sorting your rubbish before collection day. \n",
      "traditionally you shouldn't shower, wash your hair, take the rubbish out, or sweep the floor on the first day of chinese new year.\n",
      "\n",
      "Sense 1\n",
      "We charge by the amount of space that rubbish takes up.\n",
      "rubbish called\n",
      "After the 9/11 attacks, much of the debris was taken to Fresh Kills - the former rubbish dump for the city.\n",
      "rubbish called\n",
      "Bring back the days when we used to talk rubbish about men, sex and any other silly subject that made us laugh.\n",
      "rubbish called\n",
      "The three-part recycling bin makes easy work of sorting your rubbish before collection day.\n",
      "rubbish called\n",
      "Traditionally you shouldn't shower, wash your hair, take the rubbish out, or sweep the floor on the first day of Chinese New Year.\n",
      "rubbish called\n",
      "Time:  7.289076799999975\n",
      "accuracy: 1.0\n",
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def count_ones(arr):\n",
    "    count = 0\n",
    "    for val in arr:\n",
    "        if val == 1:\n",
    "            count += 1\n",
    "    return count\n",
    "filename = \"testp2.txt\"   # Replace with your file name\n",
    "lines = []                 # Initialize an empty list to store lines\n",
    "\n",
    "with open(filename, \"r\") as file:\n",
    "    for line in file:\n",
    "        lines.append(line.strip())  \n",
    "print(sents)\n",
    "print(\"Sense 1\")\n",
    "sense_1_results = []\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "for line in lines:\n",
    "    print(line)\n",
    "    sense_1_results.append(WSD_Test_Rubbish(line))\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)  \n",
    "print(f\"accuracy: {count_ones(sense_1_results)/len(sense_1_results)}\")\n",
    "print(sense_1_results)\n",
    "\n",
    "# print(\"Sense 2\")\n",
    "# sense_2_results = []\n",
    "# for sent in sense2:\n",
    "#     sense_2_results.append(WSD_Test_Tissue(sent))\n",
    "# print(f\"accuracy: {(len(sense_2_results)-count_ones(sense_2_results))/len(sense_2_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae7039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}